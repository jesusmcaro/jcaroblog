{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (1287805114.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[323], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    - date: 2024-01-10\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
     ]
    }
   ],
   "source": [
    "- title: The Virtues of a Programming your ETL Pipeline using a Fluent API\n",
    "- author: Jesus Caro\n",
    "- date: 2024-01-10\n",
    "- category: python\n",
    "- tags: docker, vscode, github, codespaces, development containers, poetry, venv\n",
    "- Subcells: [3,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from fablr.dataframes import Fablr\n",
    "from datetime import datetime as dt\n",
    "from fablr.sample_assets.artists import artists as artists_list\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DateType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "fablr = Fablr()\n",
    "fablr.set_seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id first_name  last_name                       email  last_login  \\\n",
      "0      192669   Kristina      Adams         barry06@example.com  2023-02-22   \n",
      "1      122470       Emma  Rodriguez  michaelgarrett@example.net  2023-08-03   \n",
      "2      174814     Joanna       Long         debra75@example.net  2023-08-26   \n",
      "3      191532    Michael    Johnson         debra14@example.com  2023-10-31   \n",
      "4      185296      James     Murphy    ashleythomas@example.net  2023-03-10   \n",
      "...       ...        ...        ...                         ...         ...   \n",
      "9995   122092      Holly     Hughes       cruzsarah@example.net  2023-08-06   \n",
      "9996     3103       Anne       Cruz         ukelley@example.com  2023-11-08   \n",
      "9997    45535    William     Nelson         anita93@example.org  2023-05-16   \n",
      "9998   151658  Cassandra       Cruz        deanna16@example.com  2023-09-20   \n",
      "9999    41400     Robert   Odonnell     martinjulie@example.org  2023-08-30   \n",
      "\n",
      "     subscription_tier  \n",
      "0             Standard  \n",
      "1                Prime  \n",
      "2             Standard  \n",
      "3             Standard  \n",
      "4                Prime  \n",
      "...                ...  \n",
      "9995          Standard  \n",
      "9996             Prime  \n",
      "9997             Prime  \n",
      "9998          Standard  \n",
      "9999          Standard  \n",
      "\n",
      "[10000 rows x 6 columns]\n",
      "      user_id first_name  last_name                       email  last_login  \\\n",
      "0      192669   Kristina      Adams         barry06@example.com  2023-02-22   \n",
      "1      122470       Emma  Rodriguez  michaelgarrett@example.net  2023-08-03   \n",
      "2      174814     Joanna       Long         debra75@example.net  2023-08-26   \n",
      "3      191532    Michael    Johnson         debra14@example.com  2023-10-31   \n",
      "4      185296      James     Murphy    ashleythomas@example.net  2023-03-10   \n",
      "...       ...        ...        ...                         ...         ...   \n",
      "9995    63553      Stacy      Jones     carrantonio@example.net  2023-02-19   \n",
      "9996   118676    Deborah     Gordon         awarner@example.com  2023-01-01   \n",
      "9997   189027    Antonio      Estes      amandacole@example.com  2023-10-21   \n",
      "9998   199598    Deborah    Woodard    tracysanford@example.org  2023-05-03   \n",
      "9999   114299    Matthew       Hale     jacksonkyle@example.com  2023-10-21   \n",
      "\n",
      "     subscription_tier                                               hash  \n",
      "0             Standard  225c5d45608e7c5a48262f982c0a6bda99dd5a9ecaf17c...  \n",
      "1                Prime  68f2f242a48ea055c07f72e2d1df7826d6925c8987ccec...  \n",
      "2             Standard  9e1f7f03a96b427068011ce7448f820bc31787f28165b2...  \n",
      "3             Standard  80efe0b7f32778c2318bc00e6e96e738c43d00c39ec2d8...  \n",
      "4                Prime  4e6da50edb1753e5540a92599d0ed61e87c50832d79e84...  \n",
      "...                ...                                                ...  \n",
      "9995             Prime  6535b14c1ea2213ab22dae9aa60d8658254e89e5871528...  \n",
      "9996          Standard  d688589e086d531cae2773e39f5a1d0c659b223539c1bf...  \n",
      "9997          Standard  d843e62d7e389d54943056ca2ccb68369301ca284f462d...  \n",
      "9998             Prime  dbb1cb3169a07cf2507d7751f02defbebc246749eef2e4...  \n",
      "9999          Standard  fc1df2425918dbb19755b0e99e5c2908e3e79f2d83d92b...  \n",
      "\n",
      "[9985 rows x 7 columns]\n",
      "      user_id  first_name  last_name                        email  last_login  \\\n",
      "0      192669    Kristina      Adams          barry06@example.com  2023-02-22   \n",
      "1      122470        Emma  Rodriguez   michaelgarrett@example.net  2023-08-03   \n",
      "2      174814      Joanna       Long          debra75@example.net  2023-08-26   \n",
      "3      191532     Michael    Johnson          debra14@example.com  2023-10-31   \n",
      "4      185296       James     Murphy     ashleythomas@example.net  2023-03-10   \n",
      "...       ...         ...        ...                          ...         ...   \n",
      "9995    59895    Michelle     Taylor          wparker@example.org  2023-08-13   \n",
      "9996     2213      Steven     Willis          arangel@example.com  2023-11-17   \n",
      "9997      574  Jacqueline      Welch          donna44@example.org  2023-11-21   \n",
      "9998   148831       Scott     Wilson          qharris@example.com  2023-10-19   \n",
      "9999    56962     Patrick     Norman  thorntonkathryn@example.com  2023-03-03   \n",
      "\n",
      "     subscription_tier                                               hash  \n",
      "0             Standard  225c5d45608e7c5a48262f982c0a6bda99dd5a9ecaf17c...  \n",
      "1                Prime  68f2f242a48ea055c07f72e2d1df7826d6925c8987ccec...  \n",
      "2             Standard  9e1f7f03a96b427068011ce7448f820bc31787f28165b2...  \n",
      "3             Standard  80efe0b7f32778c2318bc00e6e96e738c43d00c39ec2d8...  \n",
      "4                Prime  4e6da50edb1753e5540a92599d0ed61e87c50832d79e84...  \n",
      "...                ...                                                ...  \n",
      "9995          Standard  81fcefc0af0600eeaa194882d5052380e530a4d732ccc4...  \n",
      "9996          Standard  88cf986449b61aeba90a4baedfaeba95cc12e8d04dedfe...  \n",
      "9997          Standard  8e28c5eb829e92abf7a5a921f42364cbb8b255d7c9861a...  \n",
      "9998          Standard  012eeb49b695df5d36f586afa6a7c275a34dce8acdf82b...  \n",
      "9999          Standard  821fd5263adde96e13d51f7f198d879e8b9d7dad71fa75...  \n",
      "\n",
      "[10000 rows x 7 columns]\n",
      "           event_id        event_name  event_date         event_location  \\\n",
      "0    eid_orSA-66209        The Police  2020-10-20           Port Jessica   \n",
      "1    eid_JkoP-96029        The Eagles  2021-09-28  South Christopherside   \n",
      "2    eid_WUpA-94105          The Cure  2021-05-20           New Jennifer   \n",
      "3    eid_dICr-60851             Sting  2020-02-14              Lopezfurt   \n",
      "4    eid_ubjA-69841         Blink-182  2020-12-16             Aaronmouth   \n",
      "..              ...               ...         ...                    ...   \n",
      "795  eid_imaA-74548  Blue Oyster Cult  2020-08-02              Danaburgh   \n",
      "796  eid_nbzD-41635    Arctic Monkeys  2021-12-24           Williamville   \n",
      "797  eid_Nmyd-26561        The Smiths  2022-10-04             Port Emily   \n",
      "798  eid_voyj-83062         Aerosmith  2023-08-22         Fernandezmouth   \n",
      "799  eid_LXAy-19392           The Who  2021-03-12        Gonzalesborough   \n",
      "\n",
      "    event_result  \n",
      "0     succesfull  \n",
      "1     succesfull  \n",
      "2      cancelled  \n",
      "3     succesfull  \n",
      "4     succesfull  \n",
      "..           ...  \n",
      "795   succesfull  \n",
      "796   succesfull  \n",
      "797   succesfull  \n",
      "798   succesfull  \n",
      "799   succesfull  \n",
      "\n",
      "[800 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.cache/pypoetry/virtualenvs/jcaroblog-eKDzoHOT-py3.11/lib/python3.11/site-packages/fablr/dataframes.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(\n",
      "/home/jovyan/.cache/pypoetry/virtualenvs/jcaroblog-eKDzoHOT-py3.11/lib/python3.11/site-packages/fablr/dataframes.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ticket_no        event_id  user_id  total_charged  surcharge   taxes  \\\n",
      "0      1792862058  eid_bZzm-20159   139930         178.41       5.99  0.0500   \n",
      "1      3187359773  eid_tEoX-75047   178068         799.73       5.99  0.0925   \n",
      "2      6370507564  eid_KGDp-61012    97725         512.87      11.99  0.0500   \n",
      "3      7180894943  eid_RSpd-13750   143657         322.30       5.99  0.0800   \n",
      "4      1458571193  eid_IqmD-38722     4188         427.58      11.99  0.0800   \n",
      "...           ...             ...      ...            ...        ...     ...   \n",
      "99995  4080164684  eid_neqm-55643   172052         522.20      11.99  0.0800   \n",
      "99996  8047662809  eid_PZeq-92208   110971         406.29        NaN  0.0800   \n",
      "99997  3185907948  eid_ZANp-99972     7494         662.31        NaN  0.0800   \n",
      "99998  9201862816  eid_OXEG-92565    63337         299.78       5.99  0.0800   \n",
      "99999  8057207069  eid_owMz-63113    46603         519.50        NaN  0.0925   \n",
      "\n",
      "       hot_ticket_fees  \n",
      "0                11.99  \n",
      "1                11.99  \n",
      "2                 5.99  \n",
      "3                12.45  \n",
      "4                 5.99  \n",
      "...                ...  \n",
      "99995            11.99  \n",
      "99996             5.99  \n",
      "99997            11.99  \n",
      "99998             5.99  \n",
      "99999            12.45  \n",
      "\n",
      "[100000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "### Create dataframes\n",
    "date_format = '%Y-%m-%d'\n",
    "users_dict = {\n",
    "  'user_id': {'provider':'random_int', 'kwargs':{\"min\":0, \"max\": 2e5}},\n",
    "  'first_name': {'provider': 'first_name', 'kwargs': {}},\n",
    "  'last_name': {'provider': 'last_name', 'kwargs': {}},\n",
    "  'email': {'provider': 'email', 'kwargs': {}},\n",
    "  'last_login': {'provider': 'date_between_dates',\n",
    "                 'kwargs': {'date_start': dt.strptime('2023-01-01', date_format), 'date_end': dt.strptime('2023-12-01', date_format)}},\n",
    "  'subscription_tier' : {'provider': 'sample_list',\n",
    "                         'kwargs':{'list':[\"Prime\", \"Standard\"], 'unique': False}}\n",
    "}\n",
    "users = fablr.generate_dataframe(10000, users_dict, primary_keys=['user_id'])\n",
    "\n",
    "\n",
    "events_dict = {\n",
    "    'event_id': {'provider':'bothify', 'kwargs':{'text': 'eid_????-#####'}},\n",
    "    'event_name': {'provider': 'sample_list', 'kwargs': {'list': artists_list}},\n",
    "    'event_date': {'provider': 'date_between_dates',\n",
    "                 'kwargs': {'date_start': dt.strptime('2019-01-01', date_format), 'date_end': dt.strptime('2023-12-01', date_format)}},\n",
    "    'event_location': {'provider': 'city', 'kwargs': {}},\n",
    "    'event_result': {'provider': 'sample_list', 'kwargs': {'list': ['succesfull']*91 + ['cancelled']*9}},\n",
    "}\n",
    "events = fablr.generate_dataframe(800, events_dict)\n",
    "\n",
    "tickets_dict = {\n",
    "    'ticket_no': {'provider':'random_int', 'unique': True, 'kwargs':{\"min\":0, \"max\": 1e10}},\n",
    "    'event_id': {'provider':'sample_dataframe', 'kwargs': {'df': events, 'column': 'event_id'}},\n",
    "    'user_id': {'provider':'sample_dataframe', 'kwargs': {'df': users, 'column': 'user_id'}},\n",
    "    'total_charged': {'provider': 'random_float', 'kwargs': {'min': 20, 'max': 800}},\n",
    "    'surcharge': {'provider': 'sample_list', 'kwargs': {'list': [5.99]*20 + [11.99]*15 + [None]*15}},\n",
    "    'taxes': {'provider': 'sample_list', 'kwargs': {'list': [0.08]*20 + [0.05]*15 + [0.0925]*15}},\n",
    "    'hot_ticket_fees': {'provider': 'sample_list', 'kwargs': {'list': [5.99]*20 + [11.99]*15 + [12.45]*15}},\n",
    "}\n",
    "\n",
    "tickets = fablr.generate_dataframe(100000, tickets_dict)\n",
    "\n",
    "users_df = spark.createDataFrame(users)\n",
    "events_df = spark.createDataFrame(events).withColumn('event_date', F.col('event_date').cast('date'))\n",
    "tickets_df = spark.createDataFrame(tickets)\n",
    "root_cols = [\"ticket_no\", \"event_id\", \"user_id\"]\n",
    "tickets_df = tickets_df.select( root_cols + [F.struct(*[\"total_charged\", \"surcharge\", \"taxes\", \"hot_ticket_fees\"]).alias(\"ticket_receipt\")])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: The Fluent Interface Pattern"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week marked a new chapter in my career. I started a new job as a Data Engineer for Synaptiq building ETL pipelines that feed into ML and AI models. Looking back at my (nearly) year long career at Sequoia (A First American Startup),\n",
    "I cant help but feel grateful for the opportunity to have worked with such a talented team. I learned a lot about Data Engineering. One of the most undervalues things I learned in that role \n",
    "came in the form of a design preference from my Team lead. Often he would ask me to write additional transforms and apply them in pipelines using the **Fluent API**. I googled it, and there is some literature on the topic, but it's \n",
    "a bit technical, and not really to the point. So I decided to write this post to explain what it is, and some benefits with leveraging this design by using a simple illustration in the form of a sample Pyspark pipeline. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a `Fluent Interface?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fluent interface is a design pattern that allows you to chain methods (and in our case transformations) together in a way that is easy to read and understand. Leveraging this design pattern can make your code more readable, easier to maintain, and easier to version control. Those are not the only benefits, using a fluent interface, along with \"unit transformations\" can make your code more testable. \n",
    "\n",
    "Let's look at an example. In the following example, I have created a simple data model for a ticket reseller. The data model is simple, and contains three tables:\n",
    "\n",
    "- Users: The users table contains information about the users of the platform.\n",
    "- Tickets: The tickets table contains information about the tickets that were purchased by each user.\n",
    "- Events: The events table contains information about the events that the tickets are for.\n",
    "\n",
    "An ERD for the data model is shown below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ERD](images/fluent_api/hot_tickets_erd.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll be using building our ETL pipeline in pyspark. Pyspark has a neat method called `.transform()` that allows you to apply a transform to a dataframe. This method returns a new dataframe, and allows you to chain transformations together. First, let's print out our dataframes to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users dataframe\n",
      "+-------+----------+---------+--------------------------+----------+-----------------+\n",
      "|user_id|first_name|last_name|email                     |last_login|subscription_tier|\n",
      "+-------+----------+---------+--------------------------+----------+-----------------+\n",
      "|192669 |Kristina  |Adams    |barry06@example.com       |2023-02-22|Standard         |\n",
      "|122470 |Emma      |Rodriguez|michaelgarrett@example.net|2023-08-03|Prime            |\n",
      "|174814 |Joanna    |Long     |debra75@example.net       |2023-08-26|Standard         |\n",
      "+-------+----------+---------+--------------------------+----------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Tickets dataframe\n",
      "+----------+--------------+-------+-----------------------------+\n",
      "|ticket_no |event_id      |user_id|ticket_receipt               |\n",
      "+----------+--------------+-------+-----------------------------+\n",
      "|1792862058|eid_bZzm-20159|139930 |{178.41, 5.99, 0.05, 11.99}  |\n",
      "|3187359773|eid_tEoX-75047|178068 |{799.73, 5.99, 0.0925, 11.99}|\n",
      "|6370507564|eid_KGDp-61012|97725  |{512.87, 11.99, 0.05, 5.99}  |\n",
      "+----------+--------------+-------+-----------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Events dataframe\n",
      "+--------------+----------+----------+---------------------+------------+\n",
      "|event_id      |event_name|event_date|event_location       |event_result|\n",
      "+--------------+----------+----------+---------------------+------------+\n",
      "|eid_orSA-66209|The Police|2020-10-20|Port Jessica         |succesfull  |\n",
      "|eid_JkoP-96029|The Eagles|2021-09-28|South Christopherside|succesfull  |\n",
      "|eid_WUpA-94105|The Cure  |2021-05-20|New Jennifer         |cancelled   |\n",
      "+--------------+----------+----------+---------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Users dataframe\")\n",
    "users_df.show(3, False)\n",
    "print(\"Tickets dataframe\")\n",
    "tickets_df.show(3, False)\n",
    "print(\"Events dataframe\")\n",
    "events_df.show(3, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: that tickets_df contains a nested struct called `ticket_receipt` that contains information about the ticket purchase, such as tax, fees, surcharge, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ticket_no: long (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- ticket_receipt: struct (nullable = false)\n",
      " |    |-- total_charged: double (nullable = true)\n",
      " |    |-- surcharge: double (nullable = true)\n",
      " |    |-- taxes: double (nullable = true)\n",
      " |    |-- hot_ticket_fees: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tickets_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our task is to build an ETL pipeline that will return the number of tickets sold for events between two dates, as well as the total revenue generated for those events, by subscription tier.\n",
    "\n",
    "Let's start by listing the transformations, so that we can build our ETL pipeline in a fluent way.\n",
    "\n",
    "- 1.) Filter events between two dates, that were marked `successful` and not cancelled (these were refunded)\n",
    "- 2.) Join events and tickets on event_id to grab ticket charges.\n",
    "- 3.) Expand the receipt data (this is a nested struct column) to get the ticket charges and types for each ticket, and impute `nulls` for surcharge and hot_ticket_fees.\n",
    "- 4.) Join events and users on user_id to grab the subscription tiers.\n",
    "- 5.) Group by events, and subscription tier and sum to get the total revenue, and fees + sucharge as (fee_charges) for subscription tiers pivoted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformations:\n",
    "\n",
    "Knowing what we previously summarize, a great way to build this ETL pipeline using `The Fluent Interface` is to break the pipeline above into four separate transformations\n",
    "\n",
    "- 1.) `filter_events()`\n",
    "- 2.) `explode_ticket_receipt()`\n",
    "- 3.) `agg_revenue()`\n",
    "- 4.) `format_and_round()`\n",
    "\n",
    "With all functions named intuitively to reflect the operations performed on the dataframe within each respective function. We'll be using the `transform()` method to apply each of these functions to the dataframe, and chain them together to get an intuitive, and readable ETL pipeline.\n",
    "You can read more about the `.transform()` method using the PySpark Dataframe API [here](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.transform.html).\n",
    "\n",
    "Note the function `filter_events()` is a bit different than the others. This function takes two parameters, `start_date` and `end_date`. This is because we want to be able to filter the events dataframe by any date range. The other functions do not take any parameters, and are designed to be used in a fluent way. The parameters are passed into a nested function called _df which is then used to filter the dataframe. This was a common approach that lent itself to building a more 'aesethically pleasing' pipeline. However, you can take another approach and use .transform() to pass in parameters to the functions, *without* using nested functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_events(start_date, end_date, result_event) -> DataFrame:\n",
    "    def _df(df) -> DataFrame:\n",
    "        return (\n",
    "                df.filter(F.col('event_date')\n",
    "                          .between(start_date, end_date))\n",
    "                  .filter(F.col('event_result') == result_event)\n",
    "        )\n",
    "    return _df\n",
    "\n",
    "def explode_ticket_receipt(df: DataFrame) -> DataFrame:\n",
    "    return (df.select(\"*\", F.col(\"ticket_receipt.*\"))\n",
    "            .drop(\"ticket_receipt\")\n",
    "            .fillna(0, subset=[\"surcharge\", \"hot_ticket_fees\"])\n",
    "    )\n",
    "\n",
    "def agg_revenue(df: DataFrame) -> DataFrame:\n",
    "    return (df.withColumn(\"fees_charged\", F.col(\"surcharge\") + F.col(\"hot_ticket_fees\"))\n",
    "            .groupBy(\"event_id\", \"event_name\")\n",
    "            .pivot(\"subscription_tier\")\n",
    "            .agg(F.sum(\"fees_charged\").alias(\"fees_charges\"))\n",
    "            \n",
    "    )\n",
    "def format_and_round(df: DataFrame) -> DataFrame:\n",
    "    return (df.withColumn(\"Prime\",F.round(\"Prime\", 2))\n",
    "            .withColumn(\"Standard\",F.round(\"Standard\", 2))\n",
    "            .fillna(0, subset = [\"Prime\", \"Standard\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pipeline: \n",
    "Now that we have broken up each tranformation step, writing the pipeline is easy. All we have to do is chain the functions together using the `.transform()` method. like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------+--------+\n",
      "|event_id      |event_name      |Prime  |Standard|\n",
      "+--------------+----------------+-------+--------+\n",
      "|eid_orSA-66209|The Police      |1023.04|1081.16 |\n",
      "|eid_aflN-99103|Oasis           |1116.12|1042.84 |\n",
      "|eid_yDVG-17387|Blue Oyster Cult|967.19 |995.85  |\n",
      "+--------------+----------------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_report = (\n",
    "    tickets_df\n",
    "    .transform(explode_ticket_receipt)\n",
    "    .join(events_df, on=\"event_id\")\n",
    "    .transform(filter_events(\"2020-01-01\", \"2021-01-01\", \"succesfull\"))\n",
    "    .join(users_df, on = \"user_id\", how = \"left\")\n",
    "    .transform(agg_revenue)\n",
    "    .transform(format_and_round)\n",
    ")\n",
    "agg_report.show(3, False)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plan on using this pipeline multiple times, we can create a function that takes the start and end dates as parameters and returns the aggregated report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------+--------+\n",
      "|event_id      |event_name      |Prime  |Standard|\n",
      "+--------------+----------------+-------+--------+\n",
      "|eid_orSA-66209|The Police      |1023.04|1081.16 |\n",
      "|eid_aflN-99103|Oasis           |1116.12|1042.84 |\n",
      "|eid_yDVG-17387|Blue Oyster Cult|967.19 |995.85  |\n",
      "+--------------+----------------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_revenue_tiers(\n",
    "    tickets_df: DataFrame,\n",
    "    events_df: DataFrame,\n",
    "    users_df: DataFrame,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    ") -> DataFrame:\n",
    "\n",
    "    return (\n",
    "        tickets_df.transform(explode_ticket_receipt)\n",
    "        .join(events_df, on=\"event_id\")\n",
    "        .transform(filter_events(\"2020-01-01\", \"2021-01-01\", \"succesfull\"))\n",
    "        .join(users_df, on = \"user_id\", how = \"left\")\n",
    "        .transform(agg_revenue)\n",
    "        .transform(format_and_round)\n",
    "    )\n",
    "\n",
    "report_result_df = get_revenue_tiers(\n",
    "    tickets_df,\n",
    "    events_df,\n",
    "    users_df,\n",
    "    start_date=\"2022-07-01\",\n",
    "    end_date = \"2023-01-01\",\n",
    ")\n",
    "\n",
    "report_result_df.show(3, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing your transformations\n",
    "One of the benefits of writing your ETL using the fluent interface, is that you can write tests for each of your transformations. This allows you to test each transformation in isolation, and ensure that the output is what you expect. This will help immensely when debugging your ETL pipeline.\n",
    "In the following example I have created a test for the `explode_ticket_receipt()` function. This test will ensure that the function returns the correct number of rows, and columns, and that the resulting data after the transformation is what we expect.\n",
    "\n",
    "For the sake of brevity, I have not written tests for each of our four transformations, but I think this serves as a good illustration of how to handle testing your transformations. In the code below, I am collecting and checking whether the resulting dataframe is what we expect. If it is, it returns true. Of course you wouldn't want to do this in your test suite, you would want to use the `assertDataFrameEqual` function from the `pyspark.testing.utils` utils, if you plan to design your test suite using `pytest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "\n",
    "\n",
    "def test_explode_ticket_receipt(spark):\n",
    "  \n",
    "  test_data = [{'ticket_no': 100001,\n",
    "  'event_id': 'eid_bZzm-20159',\n",
    "  'user_id': 200001,\n",
    "  'ticket_receipt': Row(total_charged=900.00, surcharge=8.99, taxes=9.15, hot_ticket_fees=None)},\n",
    "{'ticket_no': 100002,\n",
    "  'event_id': 'eid_tEoX-75047',\n",
    "  'user_id': 200002,\n",
    "  'ticket_receipt': Row(total_charged=799.73, surcharge=None, taxes=0.0925, hot_ticket_fees=11.99)},\n",
    "{'ticket_no': 100003,\n",
    "  'event_id': 'eid_KGDp-61012',\n",
    "  'user_id': 200003,\n",
    "  'ticket_receipt': Row(total_charged=512.87, surcharge=None, taxes=0.05, hot_ticket_fees=None)},\n",
    "{'ticket_no': 100003,\n",
    "  'event_id': 'eid_KGDp-61012',\n",
    "  'user_id': 200004,\n",
    "  'ticket_receipt': Row(total_charged=512.87, surcharge=160.75, taxes=0.05, hot_ticket_fees=19.95)}]\n",
    "  \n",
    "  result_df = [{'event_id': 'eid_bZzm-20159',\n",
    "    'ticket_no': 100001,\n",
    "    'user_id': 200001,\n",
    "    'total_charged': 900.0,\n",
    "    'surcharge': 8.99,\n",
    "    'taxes': 9.15,\n",
    "    'hot_ticket_fees': 0.0},\n",
    "  {'event_id': 'eid_tEoX-75047',\n",
    "    'ticket_no': 100002,\n",
    "    'user_id': 200002,\n",
    "    'total_charged': 799.73,\n",
    "    'surcharge': 0.0,\n",
    "    'taxes': 0.0925,\n",
    "    'hot_ticket_fees': 11.99},\n",
    "  {'event_id': 'eid_KGDp-61012',\n",
    "    'ticket_no': 100003,\n",
    "    'user_id': 200003,\n",
    "    'total_charged': 512.87,\n",
    "    'surcharge': 0.0,\n",
    "    'taxes': 0.05,\n",
    "    'hot_ticket_fees': 0.0},\n",
    "  {'event_id': 'eid_KGDp-61012',\n",
    "    'ticket_no': 100003,\n",
    "    'user_id': 200004,\n",
    "    'total_charged': 512.87,\n",
    "    'surcharge': 160.75,\n",
    "    'taxes': 0.05,\n",
    "  'hot_ticket_fees': 19.95}]\n",
    "  \n",
    "  test_df = spark.createDataFrame(test_data)\n",
    "  result_df = spark.createDataFrame(result_df)\n",
    "  \n",
    "  test_df = test_df.transform(explode_ticket_receipt)\n",
    "  \n",
    "  result_df = result_df.orderBy(\"ticket_no\", \"event_id\")\n",
    "  test_df = test_df.orderBy(\"ticket_no\", \"user_id\")\n",
    "  \n",
    "  result_df = result_df.select(test_df.columns) # reorder columns to match\n",
    "  return result_df.collect() == test_df.collect() # **don't** do this for large data\n",
    "  #ssertDataFrameEqual(result_df, test_df) # do this for large data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_explode_ticket_receipt(spark)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "In this post, we learned about the Fluent Interface design pattern, and how we can use it to build a readable, and intuitive ETL pipeline. We also learned about some of the benefits of using this design pattern, such as making our code more testable, easier to maintain, debug and version control. We did that by looking at a sample data model, and a simple theoretical use case for an ETL pipeline. We then broke down the pipeline into three separate transformations, and built the pipeline using the `.transform()` method (along with some joins to add pertinent data). Furthermore, we gave a simple example of how you could make this pipeline more reusable by creating a function that takes the start and end dates as parameters, and returns the aggregated report. Finally, we discussed some of the benefits of using this design pattern, such as making our code more testable, and gave a sample test case for the `explode_ticket_receipt()` function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jcaroblog-eKDzoHOT-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9aa6aee49d60ccd2bd71d375990fb075770fa95a679ee9c2bddb85a9c9a1ba2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
